"""Unified LLM client supporting multiple providers."""

import asyncio
from typing import List, Dict, Optional
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config

# Import LLM libraries
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    import openai
except ImportError:
    openai = None

try:
    import httpx
except ImportError:
    httpx = None
from typing import Optional, Dict

class LLMClient:
    def __init__(self):
        self.provider = config.LLM_PROVIDER.lower()
        self.setup_client()

    def setup_client(self):
        """Initialize the appropriate LLM client based on provider."""
        if self.provider == "gemini":
            if not genai or not config.GEMINI_API_KEY:
                raise ValueError("Gemini configuration missing")
            genai.configure(api_key=config.GEMINI_API_KEY)
            self.model = genai.GenerativeModel(config.GEMINI_MODEL)
            
        elif self.provider == "openai":
            if not openai or not config.OPENAI_API_KEY:
                raise ValueError("OpenAI configuration missing")
            self.client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
            
        elif self.provider == "deepseek":
            if not httpx or not config.DEEPSEEK_API_KEY:
                raise ValueError("DeepSeek configuration missing")
            # DeepSeek uses OpenAI-compatible API
            self.client = openai.AsyncOpenAI(
                api_key=config.DEEPSEEK_API_KEY,
                base_url=config.DEEPSEEK_BASE_URL
            )
        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")

    async def generate_news_digest(self, articles: List[Dict]) -> str:
        """Generate a news digest from collected articles."""
        if not articles:
            return "–°–µ–≥–æ–¥–Ω—è –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

        # Prepare articles summary for prompt
        articles_text = ""
        for i, article in enumerate(articles[:5], 1):  # Limit to 5 articles
            articles_text += f"{i}. {article['title']}\n"
            articles_text += f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {article['source']}\n"
            if article.get('description'):
                articles_text += f"   –û–ø–∏—Å–∞–Ω–∏–µ: {article['description'][:200]}...\n"
            articles_text += f"   –°—Å—ã–ª–∫–∞: {article['link']}\n\n"

        prompt = f"""–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö –∏ IT –≤ –ö–∏—Ç–∞–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–ò—Å—Ö–æ–¥–Ω—ã–µ —Å—Ç–∞—Ç—å–∏:
{articles_text}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–π–¥–∂–µ—Å—Ç—É:
- –û–±—ä–µ–º: 150-200 —Å–ª–æ–≤
- –Ø–∑—ã–∫: —Ä—É—Å—Å–∫–∏–π
- –°—Ç–∏–ª—å: –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –∫—Ä–∞—Ç–∫–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ + –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã/–Ω–æ–≤–æ—Å—Ç–∏ + –∑–∞–∫–ª—é—á–µ–Ω–∏–µ
- –í–∫–ª—é—á–∏ 2-3 —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
- –î–æ–±–∞–≤—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ö–µ—à—Ç–µ–≥–∏ –≤ –∫–æ–Ω—Ü–µ (#–ö–∏—Ç–∞–π–¢–µ—Ö #IT #–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏)
- –ù–ï –≤–∫–ª—é—á–∞–π –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç

–î–∞–π–¥–∂–µ—Å—Ç:"""

        try:
            return await self._generate_text(prompt)
        except Exception as e:
            print(f"‚ùå Error generating news digest: {e}")
            return self._fallback_news_digest(articles)

    async def polish_vacancy(self, description: str) -> str:
        """Polish the vacancy description to be more professional and attractive."""
        prompt = f"""–ü—Ä–µ–æ–±—Ä–∞–∑—É–π –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:
{description}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –Ø—Å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: —Ä–æ–ª—å, –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —É—Å–ª–æ–≤–∏—è, –∫–æ–Ω—Ç–∞–∫—Ç
- –¢–æ–Ω: –¥–µ–ª–æ–≤–æ–π, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –±–µ–∑ –∫–ª–∏—à–µ
- –û–±—ä–µ–º: 120-180 —Å–ª–æ–≤
- –ë–µ–∑ —ç–º–æ–¥–∑–∏ –∏ —Ö–µ—à—Ç–µ–≥–æ–≤

–í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏:"""
        try:
            return await self._generate_text(prompt)
        except Exception as e:
            print(f"‚ùå Error polishing vacancy: {e}")
            return description

    async def polish_ad(self, ad_data: Dict) -> str:
        """Polish advertisement data into a persuasive, well-structured post (RU).
        ad_data keys: title, brand, description, offer, link, contact
        """
        title = (ad_data.get("title") or ad_data.get("ad_title") or "").strip()
        brand = (ad_data.get("brand") or ad_data.get("ad_brand") or "").strip()
        description = (ad_data.get("description") or ad_data.get("ad_description") or "").strip()
        offer = (ad_data.get("offer") or ad_data.get("ad_offer") or "").strip()
        link = (ad_data.get("link") or ad_data.get("ad_link") or "").strip()
        contact = (ad_data.get("contact") or ad_data.get("ad_contact") or "").strip()
        prompt = f"""–°—Ñ–æ—Ä–º–∏—Ä—É–π —É–±–µ–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∫–ª–∞–º–Ω—ã–π –ø–æ—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–ë—Ä–µ–Ω–¥: {brand}
–û–ø–∏—Å–∞–Ω–∏–µ: {description}
–û—Ñ—Ñ–µ—Ä: {offer}
–°—Å—ã–ª–∫–∞: {link}
–ö–æ–Ω—Ç–∞–∫—Ç: {contact}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ç–µ–∫—Å—Ç—É:
- –ö–æ—Ä–æ—Ç–∫–∏–π —Ü–µ–ø–ª—è—é—â–∏–π –ª–∏–¥, –¥–∞–ª–µ–µ 2‚Äì3 –∞–±–∑–∞—Ü–∞
- –Ø—Å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: —á—Ç–æ —ç—Ç–æ, –≤—ã–≥–æ–¥—ã/–æ—Ñ—Ñ–µ—Ä, –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é
- –¢–æ–Ω: –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –±–µ–∑ —á—Ä–µ–∑–º–µ—Ä–Ω—ã—Ö –∫–ª–∏—à–µ
- –û–±—ä–µ–º: 90‚Äì140 —Å–ª–æ–≤
- –ë–µ–∑ —ç–º–æ–¥–∑–∏ –∏ —Ö–µ—à—Ç–µ–≥–æ–≤

–í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞:"""
        try:
            return await self._generate_text(prompt)
        except Exception as e:
            print(f"‚ùå Error polishing ad: {e}")
            # Fallback: compose a minimal ad text
            parts = []
            if title:
                parts.append(f"{title}.")
            if brand:
                parts.append(f"–ë—Ä–µ–Ω–¥: {brand}.")
            if description:
                parts.append(description)
            if offer:
                parts.append(f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {offer}.")
            if link:
                parts.append(f"–°—Å—ã–ª–∫–∞: {link}")
            if contact:
                parts.append(f"–ö–æ–Ω—Ç–∞–∫—Ç: {contact}")
            return " " .join(parts).strip()

    async def normalize_vacancy_freeform(self, raw_text: str) -> str:
        """Normalize a free-form vacancy text into a unified template.
        Extract fields (–ø–æ–∑–∏—Ü–∏—è, –∫–æ–º–ø–∞–Ω–∏—è, –ª–æ–∫–∞—Ü–∏—è, –∑–∞—Ä–ø–ª–∞—Ç–∞, –æ–ø—ã—Ç, –æ–ø–∏—Å–∞–Ω–∏–µ/–∑–∞–¥–∞—á–∏,
        —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —É—Å–ª–æ–≤–∏—è, –∫–æ–Ω—Ç–∞–∫—Ç) and append a detected URL as a link line.
        Output must follow the same labeled format used in generator manual fallback.
        """
        prompt = f"""–¢—ã —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—à—å –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏:
{raw_text}

–ó–∞–¥–∞—á–∞:
- –ò–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è: –ü–æ–∑–∏—Ü–∏—è, –ö–æ–º–ø–∞–Ω–∏—è, –õ–æ–∫–∞—Ü–∏—è, –ó–∞—Ä–ø–ª–∞—Ç–∞, –û–ø—ã—Ç, –û–ø–∏—Å–∞–Ω–∏–µ/–∑–∞–¥–∞—á–∏, –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è, –£—Å–ª–æ–≤–∏—è, –ö–æ–Ω—Ç–∞–∫—Ç
- –ù–∞–π–¥–∏ URL (http/https) –≤ —Ç–µ–∫—Å—Ç–µ –∏ –¥–æ–±–∞–≤—å –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫—É: "üîó –°—Å—ã–ª–∫–∞: <url>"
- –ï—Å–ª–∏ URL –Ω–µ –Ω–∞–π–¥–µ–Ω, –ù–ï –¥–æ–±–∞–≤–ª—è–π —Å—Ç—Ä–æ–∫—É "–°—Å—ã–ª–∫–∞"
- –ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏, –¥–µ–ª–æ–≤–æ–π —Ç–æ–Ω, –±–µ–∑ –≤–æ–¥—ã –∏ –∫–ª–∏—à–µ
- –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ –î–û–õ–ñ–ï–ù —Å—Ç—Ä–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –º–µ—Ç–∫–∞–º:
  üßë‚Äçüíª –ü–æ–∑–∏—Ü–∏—è: ...
  üè¢ –ö–æ–º–ø–∞–Ω–∏—è: ...
  üìç –õ–æ–∫–∞—Ü–∏—è: ...
  üí∞ –ó–∞—Ä–ø–ª–∞—Ç–∞: ...
  üß™ –û–ø—ã—Ç: ...
  üìù –û–ø–∏—Å–∞–Ω–∏–µ: ...
  ‚úÖ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: ...
  üéÅ –£—Å–ª–æ–≤–∏—è: ...
  üì¨ –ö–æ–Ω—Ç–∞–∫—Ç: ...
  üîó –°—Å—ã–ª–∫–∞: ...   (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∞—à—ë–ª URL)
- –û–ø—É—Å–∫–∞–π –ø—É—Å—Ç—ã–µ —Ä–∞–∑–¥–µ–ª—ã, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –¥—Ä—É–≥–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤

–í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
        try:
            return await self._generate_text(prompt)
        except Exception as e:
            print(f"‚ùå Error normalizing free-form vacancy: {e}")
            # Fallback: try to extract a URL and return raw text + link line
            import re
            m = re.search(r'https?://\S+', raw_text)
            link_line = f"\nüîó –°—Å—ã–ª–∫–∞: {m.group(0)}" if m else ""
            return raw_text.strip() + link_line

    async def generate_article_summary(self, article: Dict, max_chars: Optional[int] = None) -> str:
        """Generate a Russian post for a single article with optional character limit.
        When max_chars is set, the model is asked to produce a cohesive text
        up to that character count and end on a complete sentence.
        """
        title = article.get('title', '')
        source = article.get('source', '')
        description = article.get('description', '')
        base_text = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–ò—Å—Ç–æ—á–Ω–∏–∫: {source}\n–û–ø–∏—Å–∞–Ω–∏–µ: {description[:800]}"
        length_req = "–û–±—ä–µ–º: 10‚Äì15 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π" if not max_chars else (
            f"–û–±—ä–µ–º: –¥–æ {max_chars} —Å–∏–º–≤–æ–ª–æ–≤ (–Ω–µ –ø—Ä–µ–≤—ã—à–∞–π –ª–∏–º–∏—Ç)."
        )
        end_req = "–ó–∞–≤–µ—Ä—à–∏ –Ω–∞ –ø–æ–ª–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –±–µ–∑ –æ–±—Ä—ã–≤–∞."
        prompt = f"""–ü–µ—Ä–µ–≤–µ–¥–∏ –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, —Å–¥–µ–ª–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –ø–æ—Å—Ç.

–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
{base_text}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ç–µ–∫—Å—Ç—É:
- –Ø–∑—ã–∫: —Ä—É—Å—Å–∫–∏–π (–µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º ‚Äî –ø–µ—Ä–µ–≤–µ–¥–∏)
- {length_req}
- –°—Ç–∏–ª—å: –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, –±–µ–∑ –∫–ª–∏—à–µ –∏ –≤–æ–¥—ã
- –°–æ—Ö—Ä–∞–Ω—è–π —Ñ–∞–∫—Ç—ã, —Ü–∏—Ñ—Ä—ã, –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –¥–∞—Ç –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
- –î–∞–π –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –æ–±—ä—è—Å–Ω–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ IT-—Ä—ã–Ω–∫–∞
- –ë–µ–∑ –ø—Ä—è–º—ã—Ö —Å—Å—ã–ª–æ–∫, –±–µ–∑ —Ö–µ—à—Ç–µ–≥–æ–≤, –±–µ–∑ —ç–º–æ–¥–∑–∏
- –ù–ï –¥–æ–±–∞–≤–ª—è–π –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–∞–∑–¥–µ–ª–æ–≤
- {end_req}

–í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ —Å–∞–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤)."""
        try:
            # Rough token estimate: 1 token ‚âà 3.5 chars (for OpenAI). Add margin.
            max_tokens = None
            if max_chars and self.provider in ["openai", "deepseek"]:
                est = int(max_chars / 3.5) + 40
                max_tokens = max(200, min(est, 1200))
            return await self._generate_text(prompt, max_tokens=max_tokens)
        except Exception as e:
            print(f"‚ùå Error generating article summary: {e}")
            # Fallback: sentence-aware trim of description or title
            def trim_sentencewise(txt: str, limit: int) -> str:
                import re
                txt = (txt or '').strip()
                if not txt:
                    return ''
                if not limit:
                    return txt
                parts = re.split(r'(?<=[.!?‚Ä¶])\s+', txt)
                out = ''
                for p in parts:
                    if len(out) + len(p) + 1 <= limit:
                        out = (out + ' ' + p).strip()
                    else:
                        break
                return out or txt[:limit]
            if description:
                return trim_sentencewise(description.strip(), max_chars or 1200)
            return trim_sentencewise(title, max_chars or 400)

    async def _generate_text(self, prompt: str, max_tokens: Optional[int] = None) -> str:
        """Generate text using the configured LLM provider."""
        if self.provider == "gemini":
            response = await self._generate_gemini(prompt)
        elif self.provider in ["openai", "deepseek"]:
            response = await self._generate_openai_compatible(prompt, max_tokens=max_tokens)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
        return response.strip()

    async def _generate_openai_compatible(self, prompt: str, max_tokens: Optional[int] = None) -> str:
        """Use OpenAI-compatible chat completions (OpenAI/DeepSeek)."""
        try:
            result = await self.client.chat.completions.create(
                model=(config.OPENAI_MODEL if self.provider == "openai" else config.DEEPSEEK_MODEL),
                messages=[
                    {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–∏—à–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –±–µ–∑ —Å—Å—ã–ª–æ–∫ –∏ —ç–º–æ–¥–∑–∏."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=(max_tokens or 1200)
            )
            return result.choices[0].message.content
        except Exception as e:
            raise RuntimeError(f"OpenAI-compatible error: {e}")

    def _fallback_news_digest(self, articles: List[Dict]) -> str:
        """Fallback news digest when LLM fails."""
        if not articles:
            return "üì∞ –°–µ–≥–æ–¥–Ω—è –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
        
        digest = "üì∞ **–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π IT –≤ –ö–∏—Ç–∞–µ**\n\n"
        
        for i, article in enumerate(articles[:3], 1):
            digest += f"{i}. **{article['title']}**\n"
            digest += f"   üîó [{article['source']}]({article['link']})\n\n"
        
        digest += "#–ö–∏—Ç–∞–π–¢–µ—Ö #IT #–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"
        return digest

# Global LLM client instance
_llm_client = None

def get_llm_client() -> LLMClient:
    """Get or create LLM client instance."""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client

async def test_llm():
    """Test LLM client functionality."""
    client = get_llm_client()
    
    # Test news generation
    test_articles = [
        {
            "title": "Chinese AI startup raises $100M",
            "source": "TechNode",
            "description": "A new AI company in Beijing secured major funding",
            "link": "https://example.com/1"
        }
    ]
    
    print("Testing news digest generation...")
    digest = await client.generate_news_digest(test_articles)
    print(f"Generated digest:\n{digest}")
    
    # Test vacancy polishing
    test_vacancy = "–ò—â–µ–º Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –≤ –®–∞–Ω—Ö–∞–π. –ó–∞—Ä–ø–ª–∞—Ç–∞ —Ö–æ—Ä–æ—à–∞—è. –û–ø—ã—Ç 3+ –≥–æ–¥–∞."
    print("\nTesting vacancy polishing...")
    polished = await client.polish_vacancy(test_vacancy)
    print(f"Polished vacancy:\n{polished}")

if __name__ == "__main__":
    asyncio.run(test_llm())